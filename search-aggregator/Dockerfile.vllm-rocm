# vLLM Service with ROCm for Ryzen AI MAX 395+
FROM rocm/dev-ubuntu-22.04:7.1.1-complete

# Metadata
LABEL maintainer="LibreChat"
LABEL description="vLLM inference service with ROCm for Ryzen AI MAX 395+"
LABEL version="1.0"

# Set environment variables for Radeon 8060S iGPU (gfx1151)
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
# Use Radeon iGPU (NOT the NPU - NPU is separate and unsupported on Linux)
ENV HSA_OVERRIDE_GFX_VERSION=11.5.1
ENV HIP_VISIBLE_DEVICES=0
ENV PYTORCH_ROCM_ARCH=gfx1151
ENV AMD_SERIALIZE_KERNEL=3
ENV HSA_ENABLE_SDMA=0
ENV PATH=/opt/rocm/bin:$PATH
ENV ROCM_HOME=/opt/rocm

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    curl \
    git \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip
RUN python3 -m pip install --upgrade pip

# Set working directory
WORKDIR /app

# Install PyTorch with ROCm support FIRST
# Using torch from rocm6.2 index which is the latest with official wheels
RUN pip3 install --no-cache-dir \
    torch==2.5.1+rocm6.2 \
    torchvision==0.20.1+rocm6.2 \
    --index-url https://download.pytorch.org/whl/rocm6.2

# Verify PyTorch ROCm installation
RUN python3 -c "import torch; print('PyTorch:', torch.__version__); print('Has HIP:', hasattr(torch.version, 'hip')); print('HIP version:', torch.version.hip if hasattr(torch.version, 'hip') else 'N/A')"

# Install ROCm SMI Python bindings
RUN pip3 install --no-cache-dir amdsmi

# Install other dependencies (avoid reinstalling torch)
RUN pip3 install --no-cache-dir \
    transformers \
    accelerate \
    fastapi \
    uvicorn[standard] \
    pydantic

# Install vLLM WITHOUT dependencies to avoid torch reinstall
RUN pip3 install --no-cache-dir --no-deps vllm

# Install remaining vLLM dependencies
RUN pip3 install --no-cache-dir \
    ray \
    msgspec \
    protobuf \
    einops \
    xformers \
    psutil

# Verify installation
RUN python3 -c "import torch; print('Final PyTorch:', torch.__version__)"
RUN python3 -c "import vllm; print('vLLM version:', vllm.__version__)"

# Create cache directory for models
RUN mkdir -p /root/.cache/huggingface

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=300s --retries=5 \
    CMD curl -f http://localhost:8000/v1/models || exit 1

# Default command (will be overridden by docker-compose)
CMD ["vllm", "serve", "--host", "0.0.0.0", "--port", "8000", "--device", "rocm"]
